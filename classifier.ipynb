{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import sys\n",
    "import math\n",
    "from scipy import spatial,sparse\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ff_path = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\ff_of_rep\\\\'\n",
    "path = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\'\n",
    "path_test1 = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\test1\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_test2 = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\test2\\\\'\n",
    "ff_path = path_test2 +'ff_of_rep\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path_test2 + 'candidate_sp1.pickle','rb') as f:\n",
    "    candidate_sp1 = pickle.load(f)\n",
    "with open(path_test2 + 'candidate_sp2.pickle','rb+') as f:\n",
    "    candidate_sp2 = pickle.load(f)\n",
    "with open(path_test2 + 'represent_id.pickle','rb') as f:\n",
    "    feature = pickle.load(f)\n",
    "with open(path_test2 + 'location_list_sp1.pickle','rb') as f:\n",
    "    location_sp1 = pickle.load(f)\n",
    "with open(path_test2 + 'location_list_sp2.pickle','rb') as f:\n",
    "    location_sp2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open (path + 'candidate_100.pickle','wb+') as f:\n",
    "    pickle.dump(candidate_list,f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open (path + 'candidate_100.pickle','rb') as f:\n",
    "    candidate_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# open features\n",
    "with open (path + 'represent_id.pickle','rb') as f:\n",
    "    feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_vector(candidate_list,feature):\n",
    "    \n",
    "    V = dict((c,np.zeros(len(feature),)) for c in candidate_list)\n",
    "\n",
    "    for i in range(len(feature)):  \n",
    "        with open(ff_path + str(feature[i]) + '.pickle','rb') as f:\n",
    "\n",
    "                candidate = pickle.load(f)\n",
    "\n",
    "                for c in candidate:\n",
    "                    if c in V.keys():\n",
    "                        #print('true')\n",
    "                        V[c][i] = 1 \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vector1 = construct_vector(candidate_sp1,feature)\n",
    "Vector2 = construct_vector(candidate_sp2,feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normal labeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chicago = ['chicago','Arlington Heights', 'Aurora', 'Berwyn', 'Bolingbrook', \n",
    "            'Cicero','Des Plaines', \n",
    "           'Elgin','Evanston', 'Gary', 'Hammond',\n",
    "            'Hoffman Estates', 'Joliet', 'Kenosha', 'Mount Prospect','Naperville', \n",
    "            'Oak Lawn', 'Oak Park', 'Orland Park',\n",
    "            'Palatine', 'Schaumburg','Skokie','Tinley Park','Waukegan','Wheaton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeling(location_sp,seed=0,sample_mode='normal',area = chicago):\n",
    "    \n",
    "    # select those provide labels\n",
    "    candidate_w_loc = []\n",
    "    for loc in location_sp.keys():\n",
    "        if location_sp[loc] != 'NaN':\n",
    "            candidate_w_loc.append((loc,location_sp[loc].lower())) \n",
    "    \n",
    "    if sample_mode != 'None':\n",
    "        random.seed(seed)\n",
    "        if sample_mode == 'nomral' or sample_mode == 'undersample':\n",
    "            train = random.sample(set(candidate_w_loc),1000) # random sample (try oversampling, undersampling)\n",
    "        elif sample_mode == 'oversample':\n",
    "            train = random.sample(set(candidate_w_loc),3000)\n",
    "        \n",
    "        with open(path_test2 + sample_mode + '_train.pickle','wb+') as f: # save\n",
    "            pickle.dump(train,f)\n",
    "\n",
    "        df_train = pd.DataFrame(train,columns = ['id','location'])\n",
    "    \n",
    "    else:\n",
    "        df_train = pd.DataFrame(candidate_w_loc,columns = ['id','location'])\n",
    "    \n",
    "    df_train['label']=0\n",
    "    for chi in area:\n",
    "        df_train.loc[df_train.location.str.contains(chi.lower()), 'label'] = 1 \n",
    "        \n",
    "    # save the id of these temp 1 labeled to a list, query twitter user_timeline to see their tweets\n",
    "    training_chicago_id = df_train[df_train.label ==1]['id'].tolist()\n",
    "   \n",
    "    with open(path_test2 + sample_mode + '_training_id.pickle','wb') as f:\n",
    "        pickle.dump(training_chicago_id,f)\n",
    "        \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 853\n",
      "neg 2147\n",
      "pos 579\n",
      "neg 2334\n"
     ]
    }
   ],
   "source": [
    "df_train = labeling(location_sp1,seed=0, sample_mode='oversample',area = chicago)\n",
    "print('pos',len(df_train[df_train.label==1]))\n",
    "print('neg',len(df_train[df_train.label==0]))\n",
    "\n",
    "with open(path_test2 + 'oversample_train_no_match.pickle','rb') as f: #open no information\n",
    "    no_match = pickle.load(f) \n",
    "with open(path_test2 + 'oversample_train_no_info.pickle','rb') as f: #open no information\n",
    "    no_info = pickle.load(f) \n",
    "    \n",
    "df_train.loc[df_train['id'].isin(no_match),'label'] = 0\n",
    "df_train = df_train.loc[~(df_train['id'].isin(no_info))] # remove those with no information available \n",
    "postive_num = len(df_train[df_train['label']==1])\n",
    "\n",
    "print('pos',len(df_train[df_train.label==1]))\n",
    "print('neg',len(df_train[df_train.label==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hand labeling testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 1475\n",
      "neg 3897\n"
     ]
    }
   ],
   "source": [
    "label_test = labeling(location_sp2,seed=0, sample_mode='None', area = chicago)\n",
    "print('pos',len(label_test[label_test.label==1]))\n",
    "print('neg',len(label_test[label_test.label==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 1475\n",
      "neg 3897\n",
      "pos 827\n",
      "neg 4158\n"
     ]
    }
   ],
   "source": [
    "with open(path_test2 + 'None_train_no_match.pickle','rb') as f: #open no information\n",
    "    no_match = pickle.load(f) \n",
    "with open(path_test2 + 'None_train_no_info.pickle','rb') as f: #open no information\n",
    "    no_info = pickle.load(f) \n",
    "    \n",
    "print('pos',len(label_test[label_test.label==1]))\n",
    "print('neg',len(label_test[label_test.label==0]))\n",
    "\n",
    "label_test.loc[label_test['id'].isin(no_match),'label'] = 0\n",
    "label_test = label_test.loc[~(label_test['id'].isin(no_info))] # remove those with no information available \n",
    "postive_num = len(label_test[label_test['label']==1])    \n",
    "\n",
    "print('pos',len(label_test[label_test.label==1]))\n",
    "print('neg',len(label_test[label_test.label==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150852950,\n",
       " 1508556810,\n",
       " 177307659,\n",
       " 744386991542964224,\n",
       " 57294861,\n",
       " 4384669709,\n",
       " 724009354303836160,\n",
       " 3305127954,\n",
       " 388612119,\n",
       " 32981017]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test_id = label_test['id'].tolist()\n",
    "label_test_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tricks: remove some negative samples to create a more balanced set (undersampling?)\n",
    "reject_id = random.sample(set(df_train[df_train.label ==0]['id'].tolist()),2334-3*579)\n",
    "df_train = df_train.loc[~(df_train['id'].isin(reject_id))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2334, 3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.label==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train['label'].tolist()\n",
    "id_train = df_train['id'].tolist()\n",
    "\n",
    "x_train = []\n",
    "for id in id_train:\n",
    "    x_train.append(Vector1[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up testing sets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "id_test = []\n",
    "for x in candidate_sp1:\n",
    "    if x not in id_train:\n",
    "        id_test.append(x)\n",
    "x_test = []       \n",
    "for id in id_test:\n",
    "    x_test.append(Vector1[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for id in candidate_sp2:\n",
    "    x_test.append(Vector2[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification(clf, X_train, Y_train, X_test,id_test):\n",
    "    clf.fit(X_train,Y_train)\n",
    "    target_test = clf.predict(X_test)\n",
    "    df_y_test = pd.DataFrame(id_test,columns=['id'])\n",
    "    df_y_test['label'] = target_test\n",
    "    rel_id_test = df_y_test[df_y_test['label']==1]['id'].tolist()\n",
    "    irrel_id_test = df_y_test[df_y_test['label']==0]['id'].tolist()\n",
    "    \n",
    "    return df_y_test,rel_id_test,irrel_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scores(df, label_test):\n",
    "    df_compare = df[df['id'].isin(label_test_id)]\n",
    "    df_combined = pd.merge(df_compare, label_test, on='id', how='outer')\n",
    "    \n",
    "    precision = mt.precision_score(df_combined['label_y'],df_combined['label_x'])\n",
    "    recall = mt.recall_score(df_combined['label_y'],df_combined['label_x'])\n",
    "    f1 = mt.f1_score(df_combined['label_y'],df_combined['label_x'])\n",
    "    accuracy = mt.accuracy_score(df_combined['label_y'],df_combined['label_x'])\n",
    "    \n",
    "    print('precision = ', precision)\n",
    "    print('recall = ', recall)\n",
    "    print('F1 = ', f1)\n",
    "    print('accuracy = ', accuracy)\n",
    "    \n",
    "    return [precision, recall, f1, accuracy]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_path = 'C:/Users/chenwang/Documents/courses/FA16/CS597/twitterMining/InitialSet/Football/location/test2/undersampling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.274440518257\n",
      "recall =  0.281741233374\n",
      "F1 =  0.278042959427\n",
      "accuracy =  0.757271815446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27444051825677268,\n",
       " 0.28174123337363965,\n",
       " 0.27804295942720758,\n",
       " 0.75727181544633904]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BernoulliNB()\n",
    "df,rel_id,irrel_id = classification(clf, x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_NB_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_NB_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)\n",
    "\n",
    "scores(df,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.301310043668\n",
      "recall =  0.250302297461\n",
      "F1 =  0.273447820343\n",
      "accuracy =  0.779338014042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30131004366812225,\n",
       " 0.25030229746070132,\n",
       " 0.27344782034346099,\n",
       " 0.77933801404212633]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "df,rel_id,irrel_id = classification(clf,x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_DT_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_DT_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)\n",
    "\n",
    "scores(df,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.276737967914\n",
      "recall =  0.250302297461\n",
      "F1 =  0.262857142857\n",
      "accuracy =  0.767101303912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2767379679144385,\n",
       " 0.25030229746070132,\n",
       " 0.26285714285714284,\n",
       " 0.76710130391173525]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "df,rel_id,irrel_id = classification(clf,x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_KNN_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_KNN_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)\n",
    "    \n",
    "scores(df,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.359649122807\n",
      "recall =  0.09915356711\n",
      "F1 =  0.155450236967\n",
      "accuracy =  0.821263791374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35964912280701755,\n",
       " 0.09915356711003627,\n",
       " 0.15545023696682464,\n",
       " 0.82126379137412242]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100,learning_rate=1, max_depth=1, random_state=10)\n",
    "df,rel_id,irrel_id = classification(clf,x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_GBC_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_GBC_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)\n",
    "    \n",
    "scores(df,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.326923076923\n",
      "recall =  0.205562273277\n",
      "F1 =  0.252412769117\n",
      "accuracy =  0.797993981946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32692307692307693,\n",
       " 0.20556227327690446,\n",
       " 0.2524127691165553,\n",
       " 0.79799398194583748]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), learning_rate = 'adaptive', random_state = 0,max_iter=1000)\n",
    "df,rel_id,irrel_id = classification(clf,x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_NNW_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_NNW_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)\n",
    "    \n",
    "scores(df,label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions: \n",
    "\n",
    "begining of Oct\n",
    "\n",
    "1. for initial list:\n",
    "I applied very stringent approach (only look at user profile who provides 'chicago' as their location). Reasonable? Extra learning required?\n",
    "\n",
    "2. feature selection:\n",
    "How many representative nodes do we need?\n",
    "\n",
    "3. feature ranking:\n",
    "My approach reasonable? Any improvement?\n",
    "\n",
    "4. rate limit problem\n",
    "when use the clf on test data, need to retrieve test nodes's friends_list. thus only can test a small portion once a time.\n",
    "can't be real time very slow\n",
    "\n",
    "5. is this approach generic enough? \n",
    "chicago is famous. What if it's non-famous? what if it's smaller or bigger area?\n",
    "\n",
    "6. How to utilize the query user by keyword?\n",
    "\n",
    "7. How to test the performance? \n",
    "We hae ground truth (a small portion of user will list their profile location <10%)\n",
    "\n",
    "# Things to do:\n",
    "a. SVD rank feature \n",
    "\n",
    "b. haven\\'t add prior to naive bayes\n",
    "\n",
    "c. think!\n",
    "Difference of Amin's goal and my goal\n",
    "he wants to find interest, i want to find location. I have more information than he has. How can I utilize these information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate prior\n",
    "<img src= 'prior.png'>\n",
    "\n",
    "i feel like this prior wont affect ranking so can be omit, doesnt change with Ux"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# something is wrong here\n",
    "clf = SVC()\n",
    "df,rel_id,irrel_id = classification(clf,x_train,y_train,x_test,candidate_sp2)\n",
    "with open(new_path + 'clf_SVC_test_rel.pickle','wb+') as f:\n",
    "    pickle.dump(rel_id,f)\n",
    "with open(new_path + 'clf_SVC_test_irrel.pickle','wb+') as f:\n",
    "    pickle.dump(irrel_id,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
