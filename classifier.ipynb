{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from random import randint\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ff_path = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\ff_of_rep\\\\'\n",
    "path = 'C:\\\\Users\\\\chenwang\\\\Documents\\\\courses\\\\FA16\\\\CS597\\\\twitterMining\\\\InitialSet\\\\Football\\\\location\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-8d526dc4d54f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# too big to process, sampling 1000 followers from each representative nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidate_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[0mcandidate_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vocabulary?\n",
    "filenames = [f for f in listdir(ff_path) if isfile(join(ff_path, f))]\n",
    "candidate_list = []  # use this to test my PLSA model\n",
    "\n",
    "for file in filenames:\n",
    "    try:\n",
    "        with open(ff_path + file,'rb') as f:\n",
    "            candidate = pickle.load(f)\n",
    "            \n",
    "            # too big to process, sampling 1000 followers from each representative nodes\n",
    "            for c in candidate[:1000]:\n",
    "                if c not in candidate_list:\n",
    "                    candidate_list.append(c)\n",
    "           \n",
    "    except EOFError:\n",
    "        print(file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (path + 'unique.pickle','wb+') as f:\n",
    "    pickle.dump(candidate_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (path + 'unique.pickle','rb') as f:\n",
    "    candidate_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open features\n",
    "with open (path + 'represent_id.pickle','rb') as f:\n",
    "    feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(candidate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_vector(candidate_list,feature):\n",
    "    V = dict((c,np.zeros(len(feature),)) for c in candidate_list)\n",
    "\n",
    "    for i in range(len(feature)):  \n",
    "        with open(ff_path + str(feature[i]) + '.pickle','rb') as f:\n",
    "\n",
    "                candidate = pickle.load(f)\n",
    "\n",
    "                for c in candidate:\n",
    "                    if c in V.keys():\n",
    "                        #print('true')\n",
    "                        V[c][i] = 1 \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vector = construct_vector(candidate_list,feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test block to see how sparse the matrix is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sparsity(V,feature_num):\n",
    "    for v in list(V.keys()):\n",
    "        total_feature = np.sum(V[v])\n",
    "        if total_feature >= feature_num:\n",
    "            print('user_id:', v,'\\t\\ttotal feature:', total_feature)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sparsity(Vector,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate prior\n",
    "<img src= 'prior.png'>\n",
    "\n",
    "i feel like this prior wont affect ranking so can be omit, doesnt change with Ux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path + 'prob_following_rep.pickle','rb') as f:\n",
    "    prob = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prob[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Classifier\n",
    "<img src=untitled.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rank_candidate(Vector, feature):\n",
    "    candidate_score = {}\n",
    "\n",
    "    for v in list(V.keys()):  # for each user\n",
    "        \n",
    "        score = 0\n",
    "        for i in range(len(feature)): # for each feature            \n",
    "\n",
    "            if V[v][i] == 1:  # if follow\n",
    "                score += math.log2(prob[0][i]) # ln(p(follow)) \n",
    "\n",
    "            elif V[v][i] == 0:\n",
    "                score += math.log2(prob[1][i]) # ln(p(unfollow))\n",
    "\n",
    "        candidate_score[v] = score\n",
    "    \n",
    "    candidate_score_ranked = sorted(candidate_score.items(),key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    return candidate_score_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(781149191808712704, -1758.2236764613422),\n",
       " (791068282736484352, -1762.922426437889),\n",
       " (785599640146108417, -1842.8592383852822),\n",
       " (779659530242142208, -1844.5119806159519),\n",
       " (787488684912156672, -1867.8320771088486),\n",
       " (754013832108994560, -1871.9273272390972),\n",
       " (780206098431213568, -1872.5019793483407),\n",
       " (550557628, -1876.569656086811),\n",
       " (788347210698256384, -1882.9656627997808),\n",
       " (790322349098749952, -1890.5031798600978)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_rank = rank_candidate(V, feature)\n",
    "c_rank[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.0, 49.0, 35.0, 34.0, 32.0, 30.0, 31.0, 31.0, 34.0, 31.0]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_f = []\n",
    "for c in c_rank:\n",
    "    n_f.append(np.sum(V[c[0]]))\n",
    "n_f[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (path + 'ranked_candidate_id.pickle', 'wb+') as f:\n",
    "    pickle.dump(c_rank,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions: \n",
    "\n",
    "1. for initial list:\n",
    "I applied very stringent approach (only look at user profile who provides 'chicago' as their location). Reasonable? Extra learning required?\n",
    "\n",
    "2. feature selection:\n",
    "How many representative nodes do we need?\n",
    "\n",
    "3. feature ranking:\n",
    "My approach reasonable? Any improvement?\n",
    "\n",
    "4. rate limit problem\n",
    "when use the clf on test data, need to retrieve test nodes's friends_list. thus only can test a small portion once a time.\n",
    "can't be real time very slow\n",
    "\n",
    "5. is this approach generic enough? \n",
    "chicago is famous. What if it's non-famous? what if it's smaller or bigger area?\n",
    "\n",
    "6. How to utilize the query user by keyword?\n",
    "\n",
    "7. How to test the performance? \n",
    "We hae ground truth (a small portion of user will list their profile location <10%)\n",
    "\n",
    "# Things to do:\n",
    "a. SVD rank feature \n",
    "\n",
    "b. haven\\'t add prior to naive bayes\n",
    "\n",
    "c. think!\n",
    "\n",
    "I have different goal than Amin's approach.\n",
    "--------------------------------------------------------------------------------------------------------\n",
    "representatives:\n",
    "(1) Amin's approach is very time consuming, might hit a lot of rate limit. imagine loop over 150,000+ people, query their followers to get the \"follower number\"\n",
    "(2) filter the top 200 frequent \"TF\" potential candidate by their location? might sacrifice some representatives that doesn't explicitly list their location? Still consuming, hit rate limit \n",
    "\n",
    "Difference of Amin's goal and my goal\n",
    "he wants to find interest, i want to find location. I have more information than he has. How can I utilize these information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
